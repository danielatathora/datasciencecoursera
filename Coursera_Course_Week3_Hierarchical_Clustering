# Coursera - Data Analytics Week 3 

##########################################
# Video Hierarchical clustering - Part 1
##########################################

# Uclidian Distance: SQRT( SQR(X1 - X2) + SQR(Y1 - Y2)) 
#       SQRT( SQR(A1 - A2) + SQR(B1 - B2) + ... + SQR(Z1 - Z2)) 

# Manhatten Distance: |A1 - A2| + |B1 - B2|
#      |A1 - A2| + |B1 - B2| + ... + |Z1 - Z2| 
#      so absolute sum(point1 - point2)


# Video Hierarchical clustering - Part 1
set.seed(1234)
par(mar c=(0,0,0,0))
x <- rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y <- rnorm(12, mean = rep(c(1, 2, 1), each = 4), sd = 0.2)
plot(x, y, col = "blue", pch = 19, cex = 2)
text(x + 0.05, y + 0.05, lebels - as.character(1:12))


##########################################
# Video Hierarchical clustering - Part 2
##########################################


#Â x and y created above

dataFrame <- data.frame(x = x, y = y)
dist(dataFrame)


# finding point closest together, then next set of points closest together
# and you end up with a Dendrogram

# plot a Dendrogram
distxy <- dist(dataFrame)
hClustering <- hclust(distxy)
plot(hClustering)

 dependong on where you cut the tree *horizontally can get a cluster for each datapoint or a single cluster 
 or somewhere inbetween 

##########################################
# Video Hierarchical clustering - Part 3
##########################################

myplclust <- function(hclust, lab = hclust$labels, lab.col = rep(1, length(hclust$labels)),
                      hang = 0.1, ...) {
  ## modification of plclust for plotting hclust *in colour*!
  ## Argumnents: hclust: hclust object lab: a character vector
  ## of labels of the leaves of the tree lab.col: colour for the labels;
  ## NA=default device foreground colour hang: as in hclust * plclust Side
  ## effect: A disploy of hierarchical cluster with coloured leaf labels.
  y <- rep(hclust$height, 2)
  x <- as.numeric(hclust$merge)
  y <- y[which(x < 0)]
  x <- x[which(x < 0)]
  x <- abs(x)
  y <- y[order(x)]
  x <- x[order(x)]
  plot(hclust, labels = FALSE, hang = hang, ...)
  text(x = x, y = y[hclust$order] - (max(hclust$height) * hang), labels = lab[hclust$order],
       col = lab.col[hclust$order], srt = 90, adj = c(1, 0.5), xpd = NA, ...) 
}

distxy <- dist(dataFrame)
hClustering <- hclust(distxy)
myplclust(hClustering, lab = rep(1:3, each = 4), lab.col = rep(1:3, each = 4))



Merging points - two approaches
===============================

- Average Linkage:

- Complete Linkage:


Heatmaps
========
# the heatmap function runs a Hierarchical cluster analysis on the rows of the data
# and on the columns of the data.

dataFrame <- data.frame(x = x, y = y)
set.seed(143)
dataMatrix <- as.matrix(dataFrame)[sample(1:12), ]
heatmap(dataMatrix)

Notes and further resources
===========================
- Gives an idea of the relationship between variables/observations
- The picture may be unstable
    - Change a few points
    - Have different distance 
    - Pick a differnt distance
    - Change th merging strategy
    - Change the scale of the points for one variables
- But it is deterministic
- Choosing where to cut isn't always obvious
- Should be primarily used for exploration

- Resources
    - Rafa's Distance and Clusting Video
        - Euclidian Distance: two dimensions SQRT(SQR(X1 - X2) + SQR(Y1 - Y2))
                            more dimensions:
                                SQRT(SQR(A1 - A2) + SQR(B1 - B2) + ... + SQR(Y1 - Y2) + SQR(Z1 - Z2))
        - Multi Dimensional Scaling (MDS similar to PCA) to reduce to two dimensions so it can be plotted
            in a scatterplot
            It can also be achived by running a PCA and plotted the first two priniple components
                against each other.
        
        
    - Elements of statistical learning


    # =====================================
    K-Means Clustering (Part 1)

    Can we find things that are close together?

    - How do we define close?
    
    - How do we group things?
    
    - How do we visualize the grouping?
    
    - How do we intepret theh grouping?

    
    How do we define close?
    
    - Most important step
    
        - Garbage in -> garbage Out


    - Distance of similarity
    
        - Continious - exlidean distance
    
        - Continious - correlation similarity
    
        - Binary - manhattan distance
    
    
    - Pick a distance/similarity that makes sense for your problem.


    K-means clustering

    - A partioning approach

        - Fix a number of clusters

        - Get "centroids" of each cluster

        - Assign things to closest centroid

        - Recalculate centroids

    - Requires

        - A defined distance metric

        - A number of clusters

        - An initial guess as to cluster centroids

    
    - Produces

        - Final estimate of cluster centroids

        - An assignment of each point to a cluster

K-means clustering - example
set.seed(1234)
par(mar = c(0, 0, 0, 0))
x <- rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y <- rnorm(12, mean = rep(c(1, 2, 1), each = 4), sd = 0.2)
plot(x, y, col = "blue", pch = 19, cex = 2)
text(x + 0.05, y + 0.05, labels = as.character(1:12))


# =====================================
K-Means Clustering (Part 2)

dataFrame <- data.frame(x, y )
kmeansObj <- kmeans(dataFrame, centers = 3)

names(kmeansObj)

# [1] "cluster"      "centers"      "totss"        "withinss"  "tot.withinss"
# [6] "betweenss"    "size"         "iter"         "ifault"      

kmeansObj$cluster

# kmeans()
par(mar = rep(0, 2, 4))
plot(x, y, col = kmeansObj$cluster, pch = 19, cex = 2)
points(kmeansObj$centers, col = 1:3, pch = 3, cex = 3, lwd = 3)

# Heatmaps
set.seed(1234)
dataMatrix = as.matrix(dataFrame)[sample(1:12), ]
kmeansObj2 <- kmeans(dataMatrix, centers = 3)
par(mfrow = c(1,2), mar = c(2, 4, 0.1, 0.1))
image(t(dataMatrix)[, nrow(dataMatrix):1], yaxt = "n")
image(t(dataMatrix)[, order(kmeansObj$cluster)], yaxt = "n")



# Notes and further resources

- K-means requierd a number of clusters

    - Pick by eye/intuition

    - Pick by cross validation/information theory, etc.

    - Determing the number of clusters (google this)

- K-means is not detgerministic

    - Differnt # of clusters

    - Different number of iterations

- Resources

    - Rafa's Distance and Clusting Video
        
    - Elements of statistical learning




# =====================================
# Dimension Reduction (Part 1)

# Matrix data

set.seed(12345)
par(mar = rep(0.2, 4))
dataMatrix <- matrix(rnorm(400), nrow = 40)
image(1:10, 1:40, t(dataMatrix)[, nrow(dataMatrix):1])


# Cluster the data
par(mar = rep(0.2, 4))
heatmap(dataMatrix)

# No interesting pattern

# What if we add a pattern?


set.seed(678910)
for( i in 1:40) {
  # flip a Coin
  coinFlip <- rbinom(1, size =1, prob = 0.5)
  # if coin is heads add a common pattern to that row
  if(coinFlip) {
    dataMatrix[i, ] <- dataMatrix[i, ] + rep(c(0, 3), each = 5)
  }
}

# checl Matrix data again 
par(mar = rep(0.2, 4))
image(1:10, 1:40, t(dataMatrix)[, nrow(dataMatrix):1])

# run Heatmap again
par(mar = rep(0.2, 4))
heatmap(dataMatrix)

# Patterns in rows and columns
hh <- hclust(dist(dataMatrix))
dataMatrixOrdered <- dataMatrix[hh$order, ]
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(rowMeans(dataMatrixOrdered), 40:1, , xlab = "Row Mean", ylab = "Row", pch = 19)
plot(colMeans(dataMatrixOrdered), xlab = "Column", ylab = "Column Mean", pch = 19)


# Related problems

You have multivariate variables X1, ....,Xn so X1 = (X11, .....,X1m) ? I dont get this ?

- Find a new set of multivariate variables that are uncorrelated and explain as much 
    variance as possible

- If you put all the variables together in one matrix, find the best matrix created with fewer variables
    (lower rank) that explains the originak data.

The first goal is statistical and the second goel is data compression.



Related Solutions - PCA/SVD

SVD:
If X is a martix with each variable in a column and each observation in a row then the SVD 
    is a "matrix decomposition"

                            X = UDV {power of t}  pow(UDV, t)

where the columns of U are orthogonal (left singular vectors), the columns of V are 
    orthogonal (right singular vectors) and D is a diagonal martix (singlar values).


PCA:
The principle components are equal to the right singular values if you first scale (substract
     the mean, divide by the standard deviation) the variables.



# =====================================
# Dimension Reduction (Part 2)

svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(svd1$u[, 1], 40:1, , xlab = "Row", ylab = "First left singular vector",
      pch = 19)
plot(svd1$v[, 1], xlab = "Column", ylab = "First right singular vector",
     pch = 19)


# Components of the SVD - Variable explained
par(mfrow = c(1, 2))
par(mar = c(4, 4, 0, 0))

plot(svd1$d, xlab = "Column", ylab = "Singular value", pch = 19)
plot(svd1$d^2/sum(svd1$d^2), xlab = "Column", ylab = "Prop. of variance explained",
     pch = 19)


# Relationship of the principle components

par(mfrow = c(1, 1))
svd1 <- svd(scale(dataMatrixOrdered))
pca1 <- prcomp(dataMatrixOrdered, scale = TRUE)

plot(pca1$rotarion[, 1], svd1$v[, 1], pch = 19, xlab = "Principle Component 1",
      ylab = "Right Singular Vector 1")
abline(c(0,1))

# ** issue here pca1$rotation is NULL


# Component of the SVD - varianece explained
constantMatrix <- dataMatrixOrdered*0
for( i in 1:dim(dataMatrixOrdered)[1]) { constantMatrix[i,] <- rep(c(0, 1), each=5)}
svd1 <- svd(constantMatrix)
par(mfrow = c(1, 3))
image(t(constantMatrix)[, nrow(constantMatrix):1])
plot(svd1$d, xlab = "Column", ylab = "Singular value", pch=19)
plot(svd1$d^2/sum(svd1$d^2), xlab = "Column", ylab = "Prop. of variance explained", pch=19)

# What if we add a second pattern

set.seed(678910)
for( i in 1:40) {
  # flip a Coin
  coinFlip1 <- rbinom(1, size =1, prob = 0.5)
  coinFlip2 <- rbinom(1, size =1, prob = 0.5)
  # if coin is heads add a common pattern to that row
  if(coinFlip1) {
    dataMatrix[i, ] <- dataMatrix[i, ] + rep(c(0, 5), each = 5)
  }
  if(coinFlip2) {
    dataMatrix[i, ] <- dataMatrix[i, ] + rep(c(0, 5),  5)
  }
  hh <- hclust(dist(dataMatrix))
  dataMatrixOrdered <- dataMatrix[hh$order, ]
}

# Singuler value decomposition - true patterns
svd2 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(rep(c(0,1), each = 5), pch = 19, xlab = "Column", ylab = "Pattern 1")
plot(rep(c(0,1),5), pch = 19, xlab = "Column", ylab = "Pattern 2")



# v and patterns of variace in rows
svd2 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(svd2$v[, 1], pch = 19, xlab = "Column", ylab = "First right singular vector")
plot(svd2$v[, 2], pch = 19, xlab = "Column", ylab = "Second right singular vector")


# d and variance explained
svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 2))
plot(svd1$d, xlab = "Column", ylab = "Singular value", pch = 19)
plot(svd1$d^2/sum(svd1$d^2), xlab = "Column", ylab = "Percent of variance explained",
        pch = 19)


# =====================================
# Dimension Reduction (Part 3)

# Missing values

dataMatrix2 <- dataMatrixOrdered
## Randomlty insert some missingf data
dataMatrix2[sample(1:100, size = 40, replace = FALSE)] <- NA
svd1 <- svd(scale(dataMatrix2))  ## Doesn't work 

## Error: infinite or missing values in 'x'


# imputing {impute}

library(impute)  ## Available from http://bioconductor.org
dataMatrix2 <- dataMatrixOrdered
dataMatrix2[sample(1:100, size = 40, replace = FALSE)] <- NA
dataMatrix2 <- impute.knn(dataMatrix2)
svd1 <- svd(scale(dataMatrixOrdered)); svd2 <- svd(scale(dataMatrix2))
par(mfrow=c(1,2)); plot(svd1$v[, 1], pch=19) ; plot(scv2$v[,1], pch=19)




[I newed to get the data to try this]
# Face example
load("data/face.rda")
image(t(faceData)[, nrow(faceData):1])

# Face example - variace explained
svd1 <- svd(scale(fateData));
plot(svd1$d^2/sum(svd1$d^2), pch = 19, xlab = "Singular vector", ylab = "Variance explained")


# Face example - create approximations
svd1 <- svd(scale(fateData));

## Note that %*% is matrix multiplication

# Here svd1$[1] is a constant
aprox1 <- svd1$u[, 1] %*% t(svd1$v[, 1]) * svd1$d[1]

# In these examples we need to make a diagonal matric out of d
approx5  <- svd1$u[, 1:5] %*% diag(svd1$d[1:5]) %*% t(svd1$v[, 1:5]) 
approx10 <- svd1$u[, 1:10] %*% diag(svd1$d[1:10]) %*% t(svd1$v[, 1:10]) 

# Face example - plot approximations
par(mfrow=c(1, 4))
image(t(approx1)[, nrow(approx1):1], main = "(a)")
image(t(approx5)[, nrow(approx1=5):1], main = "(b)")
image(t(approx10)[, nrow(approx10):1], main = "(c)")
image(t(faceDate)[, nrow(faceData):1], main = "(d)") ## Original data



Notes and further resources

- Scale matters

- PC's/SV's may mix real patterns

- Can be computationally intensive

- Advanced data analysis from an elementary point of view (link)

- Elements of statistical learning (link)

- Alternatives

    - Factor analysis (link)

    - Independent Component analysis (link)

    - Latent semantic analysis (link)



